# AnabolicCursor Project Structure

## Project Architecture

```
AnabolicCursor/
â”œâ”€â”€ app.py                     # ğŸš€ Entry point for uvicorn
â”œâ”€â”€ core/                      # ğŸ“ Core application components
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ app.py                 # FastAPI application factory
â”‚   â”œâ”€â”€ config.py              # Application configuration
â”‚   â””â”€â”€ routes.py              # API route definitions
â”œâ”€â”€ handlers/                  # ğŸ“ Request handlers
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ proxy_client.py        # Main proxy logic (streaming & JSON)
â”œâ”€â”€ utils/                     # ğŸ“ Utility modules
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ auth.py               # Authentication utilities
â”‚   â”œâ”€â”€ http_utils.py         # HTTP helpers and error handling
â”‚   â”œâ”€â”€ logging_utils.py      # Structured logging utilities
â”‚   â”œâ”€â”€ models.py             # Model resolution and payload sanitization
â”‚   â””â”€â”€ retry_utils.py        # Retry logic for rate limits
â”œâ”€â”€ parsers/                   # ğŸ“ Data parsing modules
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ response_logger.py    # Response logging utilities
â”‚   â””â”€â”€ response_parser.py    # OpenAI response parsing
â”œâ”€â”€ logs/                      # ğŸ“ Application logs
â”‚   â””â”€â”€ proxy.log             # Main log file (JSON formatted)
â”œâ”€â”€ loki/                      # ğŸ“ Loki configuration (BETA)
â”œâ”€â”€ promtail/                  # ğŸ“ Promtail configuration (BETA)
â”œâ”€â”€ venv/                      # ğŸ“ Python virtual environment
â”œâ”€â”€ requirements.txt           # Python dependencies
â”œâ”€â”€ docker-compose.yml         # Docker services (Loki, Grafana, Promtail)
â””â”€â”€ README.md                  # Project documentation
```

## Module Responsibilities

### ğŸš€ Entry Point
- **`app.py`**: Application entry point that imports the FastAPI app from `core.app`

### ğŸ“ Core (`core/`)
- **`app.py`**: FastAPI application factory with CORS middleware setup
- **`config.py`**: Environment-based configuration management
- **`routes.py`**: API endpoint definitions (`/v1/chat/completions`, `/v1/responses`)

### ğŸ“ Handlers (`handlers/`)
- **`proxy_client.py`**: Core proxy functionality
  - `proxy_json()` - Non-streaming requests
  - `proxy_stream()` - Streaming requests (SSE)

### ğŸ“ Utils (`utils/`)
- **`auth.py`**: Authentication handling (Bearer tokens, API keys)
- **`http_utils.py`**: HTTP utilities and error handling
- **`logging_utils.py`**: Structured JSON logging with pretty printing
- **`models.py`**: Payload sanitization for gpt-5
- **`retry_utils.py`**: Retry mechanisms with exponential backoff

### ğŸ“ Parsers (`parsers/`)
- **`response_parser.py`**: OpenAI response parsing (tool calls, choices, text)
- **`response_logger.py`**: Response logging utilities

## Data Flow

```
Client (Cursor) â†’ routes.py â†’ auth.py â†’ models.py â†’ proxy_client.py â†’ OpenAI API
                                                         â†“
                                   response_parser.py â†’ response_logger.py â†’ logs/proxy.log
```

## Configuration

Environment variables:

```bash
# OpenAI API
OPENAI_BASE_URL=https://api.openai.com
OPENAI_API_KEY=your_api_key

# Models - fixed to gpt-5
DEFAULT_MODEL=gpt-5

# Logging
MAX_LOG_TEXT=2000000

# Retry Logic
RETRY_MAX=3
RETRY_BASE_SECONDS=1.5
RETRY_MAX_SECONDS=20
```